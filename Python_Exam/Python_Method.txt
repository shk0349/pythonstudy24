ㅇ Numpy
   - mean() : 평균 / std : 절대값
   - column_stack() : 행으로 나열된 배열들을 합쳐 열방향 배열로 재구성
   - set_printoptions() : 배열의 표기방법 변경
      * precision : 배열 출력 간 소숫점 자리 표시 (Default : 8)
      * suppress : 표기법 변경
         ** True : 1.2315651 등 일반적인 소수점 표기법 사용
         ** False : 1.23e+05 등 과학적 표기법 사용)
   - unique() : 배열에서 중복된 값을 제거
   - argmax() : 가장 큰값의 인덱스 추출
   - log() : 지수형식으로 표현
   - arange() : 일정 간격으로 배열 생성
   - exp() : 시그모이드 함수 적용 (대입할 값을 음수화하여 적용)
   - array() : 배열 생성
   - reshape() : 배열 재구성

--------------------------------------------------------------------------------

ㅇ Pandas
   - info() : 열에 대한 기본정보
      * Non-Null : 누락된 값이 없음
   - describe() : 열에 대한 통계
      * mean : 평균 / std : 표준편차 / min : 최소 / max : 최대 /
               사분위수 : 데이터 순서대로 4등분한 값
               (25% : 1사분위수 / 50% : 중간값 / 75% : 3사분위 / 100% : 4사분위, max)
   - read_csv() : .csv 형식의 파일을 읽음
      * head() : 가져온 데이터의 상위 데이터 출력 (Default 값 : 5)
   - unique() : 배열에서 중복된 값을 제거

--------------------------------------------------------------------------------

ㅇ matplotlib.pyplot
   - figure() : 도표 생성
      * figsize : 도표 크기 설정
   - scatter() : 산점도 생성
   - plot() : 선 그래프 생성
   - subplot() : 서브플롯 생성
   - xlabel() : x축 지정
   - ylabel() : y축 지정
   - legend() : 범례 지정
   - title() : 그래프 Title
   - show() : 생성된 모든 도표 출력
   - imshow() : 이미지 출력
      * cmap : 컬러맵 설정 (gray : 흑백이미지)

--------------------------------------------------------------------------------

ㅇ tree
   - DecisionTreeClassifier : 결정트리
      * fit() : 적용된 값 훈련
      * score() : 훈련된 값을 이용하여 적용된 값의 확률 확인
      * feature_importances_ : 특성 중요도 (각 인수들의 합 = 1)
      * min_impurity_decrease : 최소 불순도
   - plot_tree() : 의사결정나무 시각화
      * max_depth : 최상위 노드(root_nod)를 제외한 하위노드(leaf_nod) 출력개수
      * filled : 클래스에 맞는 색 적용
      * feature_names : 특성이름 적용
      * gini : 불순도(노드에서 데이터를 분할할 기준을 정하는 값)
      * samples : 총 샘플수
      * value : True(1), False(0) 값의 숫자를 배열화하여 표시

ㅇ xgboost
   - XGBClassifier() : 히스토그램 기반 그레디언트 부스팅의 회귀버젼
      * tree_method : 메소드 종류 선정 (hist : 히스토그램 기반 그레디언트 부스팅)

ㅇ lightgbm
   - LGBMClassifier() : ms에서 만든 LightGBM^2

--------------------------------------------------------------------------------
---------------- sklearn ----------------

ㅇ model_selection
   - train_test_split() : 데이터와 타겟을 훈련세트와 테스트세트로 분리 (test_size의 default : 0.25)
   - cross_validate() : 교차검증
      * return_train_score : 점수 리턴 여부 (True나 False 사용)
   - StratifiedKFold() : KFold 분할기 (분류 모델일 경우 분할기로서 사용)
   - GridSearchCV() : 머신러닝에서 모델의 성능향상을 위해 사용하는 기법으로
                      값에 대한 경우의 수마다 예측 성능을 측정/평가하여 비교하면서
                      최적의 파라미터 값을 찾은 과정을 수행
      * n_jobs : 병렬 연산에 사용될 CPU 코어수 지정 (-1 : 모든 코어 사용)
      * _results_[][] : 최상의 검증점수
      * best_params_ : 최상의 매개변수
   - RandomizedSearchCV() : 각 반복마다 임의의 값만 대입하여 지정한 횟수만큼 평가
      * n_iter : 교차검증 수행하기 위한 샘플링 횟수
      * n_jobs : 병렬 연산에 사용될 CPU 코어수 지정 (-1 : 모든 코어 사용)
      * best_params_ : 최상의 매개변수
      * cv_results_ : 최상의 교차검증 점수
      * best_estimator_ : 최상의 성능을 갖은 모델 반환

ㅇ neighbors
   - KNeighborsClassifier : K-최근접 분류
      * fit() : 적용된 값 훈련
      * score() : 적용된 값을 훈련데이터에 적용하여 결과확인
      * predict() : 적용된 값을 훈련된 데이터에 적용하여 예측
   - KneighborsRegressor : K-최근접 회귀
      * fit() : 적용된 값 훈련
      * score() : 적용된 값을 훈련데이터에 적용하여 결과확인
      * predict() : 적용된 값을 훈련된 데이터에 적용하여 예측
      * n_neighbors : 알고리즘 기본값 조절

ㅇ preprocessing
   - StandardScaler() : 표준점수화
      * fit() : 적용된 값 훈련
      * transform() : 훈련된 값을 이용하여 적용값을 표준점수화
   - PolynomialFeatures() : 다항회귀
      * include_bias : bias(1) 값 포함 여부
      * degree : 특성 조합 수 설정
      * fit() : 적용된 값 훈련
      * transform() : 훈련된 값을 이용하여 적용값을 표준점수화

ㅇ linear_model
   - LogisticRegression : 로지스틱 회귀
      * fit() : 적용된 값 훈련
      * score() : 훈련된 값을 이용하여 적용된 값의 확률 확인
      * coef_ : 적용된 값을 훈련한 결과값의 계수(가중치)
      * intercept : 적용된 값을 훈련한 결과값의 절편
      * C : 규제값 (계수의 제곱을 적용하는 L2 규제이나, 규제양과 값은 반비례)
      * max_iter : 반복횟수 (default : 100 / 반복횟수가 부족하면 경고문구 출력되니 값을 높여 적용)
   - LinearRegression : 선형회귀
      * fit() : 적용된 값 훈련
      * predict() : 적용된 값을 훈련된 데이터에 적용하여 예측
   - SGDClassifier : 선형분류
      * loss : 손실함수 지정 (log_loss, hinge 등)
      * max_iter : epoch(반복횟수) 지정
      * score() : 훈련된 값을 이용하여 적용된 값의 확률 확인
      * partial_fit() : 적용된 값 훈련
   - Ridge() : Ridge Model
      * fit() : 적용된 값 훈련
      * score() : 적용된 값을 훈련데이터에 적용하여 결과확인
      * alpha : 규제값(계수의 제곱을 적용하는 L2 규제이고, 규제양과 값은 비례)
   - Lasso() : Lasso Model
      * fit() : 적용된 값 훈련
      * score() : 적용된 값을 훈련데이터에 적용하여 결과확인
      * alpha : 규제값 적용
      * max_iter : 최대 반복 횟수 적용 / 반복횟수 부족 시 경고 발생(ConvergenceWarning)

ㅇ ensemble
   - ExtraTreesClassifier() : Bootstrap 샘플을 사용하지 않고 전체 훈련세트 사용
      * fit() : 적용된 값 훈련
      * n_jobs : 병렬 연산에 사용될 CPU 코어수 지정 (-1 : 모든 코어 사용)
      * feature_importances_ : 특성중요도
   - RandomForestClassifier() : 전체 특성 개수의 제곱근 만큼 특성 선택
      * fit() : 적용된 값 훈련
      * n_jobs : 병렬 연산에 사용될 CPU 코어수 지정 (-1 : 모든 코어 사용)
      * feature_importances_ : 특성중요도
      * oob_score_ : oob(Out of Back / Bootstrap에 포함되지 않고 남은 샘플) 점수를 평균내어 출력 여부
   - GradientBoostingClassifier() : 깊이가 얕은 결정트리를 사용하여 이전 트리의 오차를 보완하는 방식
      * fit() : 적용된 값 훈련
      * feature_importances_ : 특성중요도
   - HistGradientBoostingClassifier() : 히스토그램 기반 그래디언트 부스팅이며, 일반 그래디언트 부스팅의 개선 버젼
      * fit() : 적용된 값 훈련

ㅇ inspection
   - permutation_importance() : 특성 중요도 확인 / 특성을 하나씩 랜덤하게 섞어 모델의 성능 변화여부 관찰
      * n_jobs : 병렬 연산에 사용될 CPU 코어수 지정 (-1 : 모든 코어 사용)
      * n_repeats : 반복 횟수

ㅇ cluster
   - KMeans() : 군집 알고리즘이 비지도학습의 평균값을 자동으로 알려줌
      * fit() : 적용된 값 훈련
      * n_clusters : 적용할 클러스터 수
      * transform() : 훈련된 값을 이용하여 적용값을 표준점수화
      * predict() : 적용된 값을 훈련된 데이터에 적용하여 예측
      * n_iter_ : 반복횟수
      * n_init : 서로 다른 군집 중심점(centroid) 최초 세팅값

---------------- scipy ----------------

ㅇ stats
   - uniform() : 주어진 범위에서 고르게 값 선택(균등 분포에서 샘플링 / 정수형)
   - randint() : 주어진 범위에서 고르게 값 선택(균등 분포에서 샘플링 / 실수형)

ㅇ special
   - expit() : sigmoid 함수 사용
   - softmax() : softmax 함수 사용

---------------- 기타 용어 ----------------

ㅇ criterion : 분할 품질을 측정하는 기능 (default : gini)
ㅇ splitter : 각 노드에서 분할을 선택하는데 사용되는 전략 (default : best)
ㅇ max_depth : 트리의 최대 깊이(값이 클수록 모델의 복잡도 상승)
ㅇ min_samples_split : 자식 노드를 분할하는데 필요한 최소 샘플의 수 (default : 2)
ㅇ min_samples_leaf : 리프 노드에 있어야 할 최소 샘플 수 (default : 1)
ㅇ min_weight_fraction_leaf : min_samples_leaf와 같지만 가중치가 부여된 샘플 수에서의 비율
ㅇ max_features : 각 노드에서 분할에 사용할 특징의 최대 수
ㅇ random_state : 난수 seed 설정
ㅇ max_leaf_nodes : 리프 노드의 최대 수
ㅇ min_impurity_decrease : 최소 불순도
ㅇ min_impurity_split : 나무 성장을 멈추기 위한 임계치
ㅇ class_weight : 클래스 가중치
ㅇ presort : 데이터 정렬 필요 여부
ㅇ url : 다운로드할 파일의 url
ㅇ output_file : 다운로드 할 파일명
ㅇ ratio : 비율